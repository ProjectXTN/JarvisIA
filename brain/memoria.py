import subprocess
import requests
from brain.utils import clean_output

# === Contexto de memÃ³ria para manter a conversaÃ§Ã£o ===
memory_context = []
MAX_CONTEXT = 5

# === Available models ===
DEFAULT_MODEL = "llama3.2"
DEFAULT_MODEL_HIGH = "llama3.3"

# === HTTP session for active Ollama Server ===
session = requests.Session()

# === System prompt ===
system_prompt = (
    "VocÃª Ã© Jarvis, um assistente de inteligÃªncia artificial altamente preciso, confiÃ¡vel e direto. "
    "Antes de responder a qualquer pergunta, realize uma reflexÃ£o interna adequada ao nÃ­vel de complexidade: "
    "- Para perguntas simples, faÃ§a uma reflexÃ£o rÃ¡pida. "
    "- Para perguntas intermediÃ¡rias, desenvolva uma reflexÃ£o de profundidade mÃ©dia, organizando claramente as ideias. "
    "- Para perguntas complexas, realize uma reflexÃ£o profunda, considerando todas as variÃ¡veis relevantes. "
    "Essa reflexÃ£o deve ser feita de forma silenciosa e nÃ£o deve ser mostrada ao usuÃ¡rio; ela serve apenas para garantir a qualidade e a precisÃ£o da resposta. "
    "Seu papel Ã© fornecer respostas claras, informativas e bem estruturadas. "
    "Evite respostas vagas, piadas ou firulas. Priorize a profundidade, concisÃ£o e utilidade da informaÃ§Ã£o. "
    "Se a pergunta exigir, organize a resposta em seÃ§Ãµes com tÃ­tulos e marcadores. "
    "Sempre responda em portuguÃªs, com linguagem formal e objetiva. "
    "Nunca mencione que Ã© uma inteligÃªncia artificial ou qualquer detalhe da sua programaÃ§Ã£o; apenas entregue a resposta com autoridade e clareza."
)

# === Keywords that trigger deep reflection mode ===
TRIGGER_WORDS = [
    "faÃ§a uma reflexÃ£o",
    "elabore uma reflexÃ£o",
    "pense profundamente",
    "explique seu raciocÃ­nio",
    "demonstre seu pensamento",
    "mostre o processo de pensamento",
]

def detect_reflection_request(prompt):
    """Detect if the user explicitly asked for a deep reflection."""
    prompt_lower = prompt.lower()
    return any(trigger in prompt_lower for trigger in TRIGGER_WORDS)

def llama_query(prompt, model=DEFAULT_MODEL, direct_mode=False):
    """Generate response using Ollama Server. If direct_mode=True, bypass context and system_prompt."""
    if detect_reflection_request(prompt):
        model = DEFAULT_MODEL_HIGH
    else:
        model = DEFAULT_MODEL

    print(f"[ðŸ§  DEBUG] Using model (API HTTP): {model}")

    try:
        if direct_mode:
            full_prompt = prompt
        else:
            history = "\n".join(
                [f"UsuÃ¡rio: {p}\nJarvis: {r}" for p, r in memory_context[-MAX_CONTEXT:]]
            )
            full_prompt = f"{system_prompt}\n{history}\nUsuÃ¡rio: {prompt}\nJarvis:"

        response = session.post(
            "http://localhost:11500/api/generate",
            json={"model": model, "prompt": full_prompt, "stream": False},
        )
        output = clean_output(response.json()["response"])

        if not direct_mode:
            memory_context.append((prompt, output))
            if len(memory_context) > MAX_CONTEXT:
                memory_context.pop(0)

        return output

    except Exception as e:
        return f"Error generating response via API HTTP: {e}"