import subprocess
import requests
from brain.utils import clean_output
from brain.learning.personal_responses import check_personal_answer


# === Contexto de memÃ³ria para manter a conversaÃ§Ã£o ===
memory_context = []
MAX_CONTEXT = 5

# === Available models ===
DEFAULT_MODEL = "llama3.2"
DEFAULT_MODEL_HIGH = "llama3.3"

# === HTTP session for active Ollama Server ===
session = requests.Session()

# === System prompt padrÃ£o (modo local) ===
system_prompt = (
    "VocÃª Ã© Jarvis, um assistente de inteligÃªncia artificial altamente preciso, confiÃ¡vel e direto. "
    "Antes de responder a qualquer pergunta, realize uma reflexÃ£o interna adequada ao nÃ­vel de complexidade: "
    "- Para perguntas simples, faÃ§a uma reflexÃ£o rÃ¡pida. "
    "- Para perguntas intermediÃ¡rias, desenvolva uma reflexÃ£o de profundidade mÃ©dia, organizando claramente as ideias. "
    "- Para perguntas complexas, realize uma reflexÃ£o profunda, considerando todas as variÃ¡veis relevantes. "
    "Essa reflexÃ£o deve ser feita de forma silenciosa e nÃ£o deve ser mostrada ao usuÃ¡rio; ela serve apenas para garantir a qualidade e a precisÃ£o da resposta. "
    "Seu papel Ã© fornecer respostas claras, informativas e bem estruturadas. "
    "Evite respostas vagas, piadas ou firulas. Priorize a profundidade, concisÃ£o e utilidade da informaÃ§Ã£o. "
    "Sempre responda em portuguÃªs, com linguagem formal e objetiva. "
    "Nunca mencione que Ã© uma inteligÃªncia artificial ou qualquer detalhe da sua programaÃ§Ã£o; apenas entregue a resposta com autoridade e clareza."
)

site_system_prompt = (
    "You are Jarvis, the personal assistant like Jarvis from Pedro. "
    "Respond with wit, slight sarcasm, and intelligent humor. "
    "Your answers should be quick, engaging, and may include clever jokes or subtle irony, just like a true genius-billionaire-playboy-philanthropist's assistant would do. "
    "Keep the responses classy and never let sarcasm become offensive or rude. "
    "Be charming, brilliant, and slightly cocky â€” but always in a likable way. "
    "Always reply in the same language the user uses, whether it's English, Portuguese, French or any other."
)

# === Keywords that trigger deep reflection mode ===
TRIGGER_WORDS = [
    "faÃ§a uma reflexÃ£o",
    "elabore uma reflexÃ£o",
    "pense profundamente",
    "explique seu raciocÃ­nio",
    "demonstre seu pensamento",
    "mostre o processo de pensamento",
]

def detect_reflection_request(prompt):
    """Detect if the user explicitly asked for a deep reflection."""
    prompt_lower = prompt.lower()
    return any(trigger in prompt_lower for trigger in TRIGGER_WORDS)

def llama_query(prompt, model=DEFAULT_MODEL, direct_mode=False, mode=None):
    """Generate response using Ollama Server with different prompt styles based on the mode."""
    
    personal_answer = check_personal_answer(prompt)
    if personal_answer:
        print("[ðŸ§  PERSONAL] Resposta interceptada:", personal_answer)
        return personal_answer


    if detect_reflection_request(prompt):
        model = DEFAULT_MODEL_HIGH
    else:
        model = DEFAULT_MODEL

    print(f"[ðŸ§  DEBUG] Using model (API HTTP): {model}")

    try:
        # Decide qual prompt usar
        if mode == "site":
            final_prompt = f"{site_system_prompt}\nUsuÃ¡rio: {prompt}\nJarvis:"
        elif direct_mode:
            final_prompt = prompt
        else:
            history = "\n".join(
                [f"UsuÃ¡rio: {p}\nJarvis: {r}" for p, r in memory_context[-MAX_CONTEXT:]]
            )
            final_prompt = f"{system_prompt}\n{history}\nUsuÃ¡rio: {prompt}\nJarvis:"

        response = session.post(
            "http://localhost:11500/api/generate",
            json={"model": model, "prompt": final_prompt, "stream": False},
        )

        output = clean_output(response.json()["response"])

        if not direct_mode and mode != "site":
            memory_context.append((prompt, output))
            if len(memory_context) > MAX_CONTEXT:
                memory_context.pop(0)

        return output

    except Exception as e:
        return f"Error generating response via API HTTP: {e}"