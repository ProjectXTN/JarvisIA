import re
from brain.learning.consultar_memoria import consultar_memoria, consultar_tudo
from brain.memoria import DEFAULT_MODEL,DEFAULT_MODEL_HIGH, generate_response


def generate_contextual_response(question, model=DEFAULT_MODEL):
    # Detect if the question is about programming code
    is_code_request = any(p in question.lower() for p in [
        "c√≥digo", "script", "fun√ß√£o", "programa",
        "html", "css", "javascript", "python",
        "classe",
    ])

    # If it's a code request, force the high model (LLaMA 3.3)
    model = DEFAULT_MODEL_HIGH if is_code_request else model

    match = re.search(r"sobre\s+(.+)", question.lower())
    subject = match.group(1).strip() if match else question.strip().lower()

    knowledge = consultar_memoria(subject)

    if knowledge:
        print(f"[üîç CONTEXT] Pre-existing knowledge found about '{subject}'.")

    if is_code_request:
        prompt = (
            f"Gere apenas o c√≥digo-fonte para: {question}\n"
            f"N√£o adicione coment√°rios, explica√ß√µes, t√≠tulo, introdu√ß√£o ou conclus√£o.\n"
            f"Use quebras de linha, indenta√ß√£o e escopo correto. Escreva o c√≥digo como se fosse colado diretamente num editor de c√≥digo.\n"
            f"N√£o coloque a linguagem ('python', 'javascript', etc.) no in√≠cio.\n"
            f"Retorne apenas um √∫nico bloco de c√≥digo v√°lido entre crases triplas (```), sem texto fora dele."
        )
    else:
        prompt = (
            f"Voc√™ √© Jarvis, um assistente que responde com base no que j√° aprendeu.\n\n"
            f"Conhecimento armazenado sobre '{subject}':\n{knowledge}\n\n"
            f"Pergunta: {question}\n"
            f"Responda de forma clara, objetiva e apenas com base no que voc√™ sabe.\n"
        )

    return generate_response(prompt, model)

def respond_with_inference(question, model=DEFAULT_MODEL):
    is_code_request = any(p in question.lower() for p in ["c√≥digo", "script", "fun√ß√£o", "programa", "html", "css", "javascript", "python", "classe", "crie", "fa√ßa"])

    if is_code_request:
        prompt = (
            f"Gere apenas o c√≥digo-fonte para: {question}\n"
            f"N√£o adicione coment√°rios, explica√ß√µes, t√≠tulo, introdu√ß√£o ou conclus√£o.\n"
            f"Retorne apenas o bloco de c√≥digo entre crases triplas no formato da linguagem."
        )
        return generate_response(prompt, model)

    topics = re.findall(
        r"\b(?:intelig√™ncia artificial|rob√≥tica|blockchain|energia renov√°vel|biotecnologia|computa√ß√£o qu√¢ntica|aumento da popula√ß√£o)\b",
        question.lower()
    )
    topics = list(set(topics))

    if not topics:
        print("[üß† INFERENCE] No topic identified in the question. Using direct model as final fallback.")
        prompt = (
            f"Voc√™ √© Jarvis, um assistente que responde perguntas com base em conhecimento amplo.\n"
            f"Pergunta: {question}\n"
            f"Responda de forma objetiva e clara em portugu√™s."
        )
        return generate_response(prompt, model)

    knowledge_base = []
    for topic in topics:
        content = consultar_memoria(topic)
        if content:
            knowledge_base.append((topic, content))
            print(f"[üß† INFERENCE] Knowledge found: '{topic}'")
        else:
            print(f"[‚ùå INFERENCE] No saved data about: '{topic}'")

    if not knowledge_base:
        print("[üß† INFERENCE] No knowledge base found. Using direct model as final fallback.")
        prompt = (
            f"Voc√™ √© Jarvis, um assistente que responde perguntas com base em conhecimento amplo.\n"
            f"Pergunta: {question}\n"
            f"Responda de forma objetiva e clara em portugu√™s."
        )
        return generate_response(prompt, model)

    context = "\n\n".join(
        f"[{title.upper()}]\n{text}" for title, text in knowledge_base
    )

    prompt = (
        f"Voc√™ √© Jarvis, um assistente que responde com base no que j√° aprendeu.\n\n"
        f"Abaixo est√£o informa√ß√µes que voc√™ j√° aprendeu:\n\n{context}\n\n"
        f"Pergunta: {question}\n"
        f"Use o que aprendeu para responder de forma objetiva e em portugu√™s."
    )

    print(f"[üß† INFERENCE] Generating response based on multiple knowledge entries...")
    return generate_response(prompt, model)