import re
from brain.learning.consultar_memoria import consultar_memoria, consultar_tudo
from brain.memoria import DEFAULT_MODEL, DEFAULT_MODEL_HIGH, llama_query

def is_code_request(question):
    """Detect if the question is requesting code."""
    keywords = ["c√≥digo", "script", "fun√ß√£o", "programa", "html", "css", "javascript", "python", "classe", "crie", "fa√ßa"]
    return any(word in question.lower() for word in keywords)

def find_best_topic(question):
    """Find the best matching topic from the knowledge base."""
    all_knowledge = consultar_tudo()
    question_lower = question.lower()
    
    for title, _ in all_knowledge:
        if title.lower() in question_lower:
            print(f"[üîç MATCH] Topic match found: '{title}'")
            return title
    print("[üîé MATCH] No specific topic found in knowledge base.")
    return None

def generate_dynamic_contextual_response(question, model=DEFAULT_MODEL_HIGH):
    """Generate a response dynamically using existing knowledge if possible."""
    # Detect code request
    if is_code_request(question):
        model = DEFAULT_MODEL_HIGH
        prompt = (
            f"Gere apenas o c√≥digo-fonte para: {question}\n"
            f"N√£o adicione coment√°rios, explica√ß√µes, t√≠tulo, introdu√ß√£o ou conclus√£o.\n"
            f"Use quebras de linha, indenta√ß√£o e escopo correto. Escreva o c√≥digo como se fosse colado diretamente num editor de c√≥digo.\n"
            f"N√£o coloque a linguagem ('python', 'javascript', etc.) no in√≠cio.\n"
            f"Retorne apenas um √∫nico bloco de c√≥digo v√°lido entre crases triplas (```), sem texto fora dele."
        )
        return llama_query(prompt, model)

    # Try to match a topic from knowledge base
    matched_topic = find_best_topic(question)

    if matched_topic:
        knowledge = consultar_memoria(matched_topic)
        if knowledge:
            print(f"[üß† CONTEXT] Knowledge found for topic: '{matched_topic}'.")
            content = knowledge[0]
            prompt = (
                f"Voc√™ √© Jarvis, um assistente que responde com base no que j√° aprendeu.\n\n"
                f"Conhecimento armazenado sobre '{matched_topic}':\n{content}\n\n"
                f"Pergunta: {question}\n"
                f"Responda de forma clara, objetiva e apenas com base no que voc√™ sabe."
            )
            return llama_query(prompt, model)
        else:
            print(f"[‚ùå CONTEXT] Topic matched but no content found for: '{matched_topic}'.")

    # Fallback if no contextual match found
    print("[üß† CONTEXT] No contextual match found. Using fallback prompt.")
    prompt = (
        f"Voc√™ √© Jarvis, um assistente que responde perguntas com base em conhecimento amplo.\n"
        f"Pergunta: {question}\n"
        f"Responda de forma objetiva e clara em portugu√™s."
    )
    return llama_query(prompt, model)
