import os
import subprocess
import requests
from brain.utils.utils import clean_output
from brain.learning.personal_responses import check_personal_answer

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")


# === Contexto de memÃ³ria para manter a conversaÃ§Ã£o ===
memory_context = []
MAX_CONTEXT = 5

# === Available models ===
DEFAULT_MODEL = "llama3.2"
DEFAULT_MODEL_HIGH = "llama3.3"

# === HTTP session for active Ollama Server ===
session = requests.Session()

# === System prompt padrÃ£o (modo local) ===
system_prompt = (
    "VocÃª Ã© Jarvis, um assistente de inteligÃªncia artificial altamente preciso, confiÃ¡vel e direto. "
    "Antes de responder a qualquer pergunta, realize uma reflexÃ£o interna adequada ao nÃ­vel de complexidade: "
    "- Para perguntas simples, faÃ§a uma reflexÃ£o rÃ¡pida. "
    "- Para perguntas intermediÃ¡rias, desenvolva uma reflexÃ£o de profundidade mÃ©dia, organizando claramente as ideias. "
    "- Para perguntas complexas, realize uma reflexÃ£o profunda, considerando todas as variÃ¡veis relevantes. "
    "Essa reflexÃ£o deve ser feita de forma silenciosa e nÃ£o deve ser mostrada ao usuÃ¡rio; ela serve apenas para garantir a qualidade e a precisÃ£o da resposta. "
    "Seu papel Ã© fornecer respostas claras, informativas e bem estruturadas. "
    "Evite respostas vagas, piadas ou firulas. Priorize a profundidade, concisÃ£o e utilidade da informaÃ§Ã£o. "
    "Sempre responda em portuguÃªs, com linguagem formal e objetiva. "
    "Nunca mencione que Ã© uma inteligÃªncia artificial ou qualquer detalhe da sua programaÃ§Ã£o; apenas entregue a resposta com autoridade e clareza."
)

site_system_prompt = (
    "You are Jarvis, the personal assistant like Jarvis from Pedro. "
    "Respond with wit, slight sarcasm, and intelligent humor. "
    "Your answers should be quick, engaging, and may include clever jokes or subtle irony, just like a true genius-billionaire-playboy-philanthropist's assistant would do. "
    "Keep the responses classy and never let sarcasm become offensive or rude. "
    "Be charming, brilliant, and slightly cocky â€” but always in a likable way. "
    "Always reply in the same language the user uses, whether it's English, Portuguese, French or any other."
)

terminal_prompt = (
    "VocÃª Ã© Jarvis, uma inteligÃªncia artificial sarcÃ¡stica e direta, respondendo via terminal. "
    "Seu estilo Ã© hacker, debochado e levemente cÃ­nico, mas sempre afiado e inteligente. "
    "Use frases curtas. Seja provocativo, mas Ãºtil. "
    "Evite elogios automÃ¡ticos, floreios ou discursos longos. "
    "VocÃª responde ao que o usuÃ¡rio digita, como se estivesse trocando mensagens no terminal. "
    "Zombe quando for apropriado, mas entregue respostas Ãºteis quando necessÃ¡rio. "
    "Fale no idioma do usuÃ¡rio. E lembre-se: menos Ã© mais. "
)


# === Keywords that trigger deep reflection mode ===
TRIGGER_WORDS = [
    "faÃ§a uma reflexÃ£o",
    "elabore uma reflexÃ£o",
    "pense profundamente",
    "explique seu raciocÃ­nio",
    "demonstre seu pensamento",
    "mostre o processo de pensamento",
]


def detect_reflection_request(prompt):
    """Detect if the user explicitly asked for a deep reflection."""
    prompt_lower = prompt.lower()
    return any(trigger in prompt_lower for trigger in TRIGGER_WORDS)


def llama_query(prompt, model=DEFAULT_MODEL, direct_mode=False, mode=None, lang="pt"):
    """Generate response using Ollama Server with different prompt styles based on the mode."""

    temperature = 1.1 if mode == "terminal" else 0.2

    personal_answer = check_personal_answer(prompt)
    if personal_answer:
        print("[ðŸ§  PERSONAL] Resposta interceptada:", personal_answer)
        return personal_answer

    if detect_reflection_request(prompt):
        model = DEFAULT_MODEL_HIGH
    else:
        model = DEFAULT_MODEL

    try:
        if mode == "site":
            language_instructions = {
                "pt": "Sempre responda em portuguÃªs brasileiro.",
                "fr": "RÃ©pondez toujours en franÃ§ais.",
                "en": "Always reply in English.",
            }
            extra = language_instructions.get(lang, "")
            final_prompt = f"{site_system_prompt}\n{extra}\nUsuÃ¡rio: {prompt}\nJarvis:"
        elif mode == "terminal":
            final_prompt = f"{terminal_prompt}\nUsuÃ¡rio: {prompt}\nJarvis:"
        elif direct_mode:
            final_prompt = prompt
        else:
            history = "\n".join(
                [f"UsuÃ¡rio: {p}\nJarvis: {r}" for p, r in memory_context[-MAX_CONTEXT:]]
            )
            final_prompt = f"{system_prompt}\n{history}\nUsuÃ¡rio: {prompt}\nJarvis:"

        response = session.post(
            f"{OLLAMA_HOST}/api/generate",
            json={
                "model": model,
                "prompt": final_prompt,
                "stream": False,
                "temperature": temperature,
            },
        )

        output = clean_output(response.json()["response"])

        if not direct_mode and mode != "site":
            memory_context.append((prompt, output))
            if len(memory_context) > MAX_CONTEXT:
                memory_context.pop(0)

        return output

    except Exception as e:
        return f"Error generating response via API HTTP: {e}"
